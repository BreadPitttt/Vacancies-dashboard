name: Data Pipeline

on:
  workflow_dispatch:
    inputs:
      force_mode:
        description: "Optional: force run mode (nightly|weekly)"
        required: false
        default: ""
  schedule:
    - cron: '17 18 * * *'
    - cron: '28 20 * * 0'

permissions:
  contents: write
  issues: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 240
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Decide mode (nightly or weekly)
        id: decide
        shell: bash
        run: |
          MODE="nightly"; TIMEOUT=120; IS_WEEKLY=false
          if [ "${{ github.event_name }}" = "schedule" ]; then
            if [[ "${{ github.event.schedule }}" == *"28 20"* ]]; then
              MODE="weekly"; TIMEOUT=240; IS_WEEKLY=true
            fi
          fi
          if [ -n "${{ github.event.inputs.force_mode }}" ]; then
            fm="${{ github.event.inputs.force_mode }}"
            if [ "$fm" = "weekly" ]; then MODE="weekly"; TIMEOUT=240; IS_WEEKLY=true; fi
            if [ "$fm" = "nightly" ]; then MODE="nightly"; TIMEOUT=120; IS_WEEKLY=false; fi
          fi
          echo "mode=$MODE" >> "$GITHUB_OUTPUT"
          echo "timeout=$TIMEOUT" >> "$GITHUB_OUTPUT"
          echo "is_weekly=$IS_WEEKLY" >> "$GITHUB_OUTPUT"
          if $IS_WEEKLY; then
            echo "run_label=Weekly hard recheck" >> "$GITHUB_OUTPUT"
          else
            echo "run_label=Nightly recheck" >> "$GITHUB_OUTPUT"
          fi

      - name: Set up Python and cache dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        id: scrape
        env:
          PYTHONUNBUFFERED: "1"
          IS_HARD_RECHECK: ${{ steps.decide.outputs.is_weekly }}
          TELEGRAM_RSS_BASE: ${{ secrets.TELEGRAM_RSS_BASE }}
          MAX_WORKERS: ${{ vars.MAX_WORKERS }}
          PER_SOURCE_MAX: ${{ vars.PER_SOURCE_MAX }}
          CONNECT_TIMEOUT: ${{ vars.CONNECT_TIMEOUT }}
          READ_TIMEOUT: ${{ vars.READ_TIMEOUT }}
          RETRY_TOTAL: ${{ vars.RETRY_TOTAL }}
          RETRY_CONNECT: ${{ vars.RETRY_CONNECT }}
          RETRY_READ: ${{ vars.RETRY_READ }}
          BACKOFF_FACTOR: ${{ vars.BACKOFF_FACTOR }}
          MAX_BACKOFF_SECONDS: ${{ vars.MAX_BACKOFF_SECONDS }}
          PER_HOST_RPM: ${{ vars.PERM_HOST_RPM || vars.PER_HOST_RPM }}
          BASELINE_SLEEP_S: ${{ vars.BASELINE_SLEEP_S }}
          JITTER_MIN: ${{ vars.JITTER_MIN }}
          JITTER_MAX: ${{ vars.JITTER_MAX }}
          CB_FAILURE_THRESHOLD: ${{ vars.CB_FAILURE_THRESHOLD }}
          CB_OPEN_SECONDS: ${{ vars.CB_OPEN_SECONDS }}
          CB_HALF_OPEN_PROBE: ${{ vars.CB_HALF_OPEN_PROBE }}
          MULTI_AGG_DAYS: ${{ vars.MULTI_AGG_DAYS }}
        run: |
          set -e
          echo "Running mode=${{ steps.decide.outputs.mode }} with per-step timeout=${{ steps.decide.outputs.timeout }}m"
          timeout "${{ steps.decide.outputs.timeout }}m" python scraper.py

      - name: Validate JSON Schema
        id: validate
        run: |
          set -e
          python validate.py

      - name: Quality checks (links, dates, dups)
        id: qc
        run: |
          set +e
          python qc_checks.py
          echo "qc_exit=$?" >> $GITHUB_OUTPUT

      - name: Generate and Format Data Files with Health
        id: health
        run: |
          set -e
          python - << 'PY'
          import json, datetime, pathlib, os
          data_path = pathlib.Path("data.json")
          health_path = pathlib.Path("health.json")
          ok = False
          reason = None
          qc_exit = os.environ.get("QC_EXIT", "0")
          data = {"jobListings": [], "transparencyInfo": {}}
          try:
            if data_path.exists():
              raw = data_path.read_text(encoding="utf-8")
              data = json.loads(raw) if raw.strip() else {"jobListings": [], "transparencyInfo": {}}
            size_ok = len(json.dumps(data).encode("utf-8")) > 200
            ok = (qc_exit == "0") and size_ok and isinstance(data.get("jobListings"), list)
            reason = None if ok else f"qc_exit={qc_exit}, size_ok={size_ok}"
          except Exception as e:
            reason = f"read_error: {e}"
          ti = data.get("transparencyInfo", {}) or {}
          ti.setdefault("totalListings", len(data.get("jobListings") or []))
          ti.setdefault("lastUpdated", datetime.datetime.utcnow().isoformat() + "Z")
          health = {
            "ok": bool(ok),
            "reason": reason,
            "checkedAt": datetime.datetime.utcnow().isoformat() + "Z",
            **ti
          }
          health_path.write_text(json.dumps(health, indent=2, ensure_ascii=False), encoding="utf-8")
          data_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
          PY
        env:
          QC_EXIT: ${{ steps.qc.outputs.qc_exit }}

      - name: Upload artifacts (data.json, health.json)
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.decide.outputs.mode }}-artifacts
          path: |
            data.json
            health.json
          retention-days: 7

      - name: Commit and Push Changes (scheduled only)
        if: github.event_name == 'schedule'
        run: |
          set -e
          git config user.name "GitHub Actions Bot"
          git config user.email "actions-bot@users.noreply.github.com"
          if ! git diff --quiet -- data.json health.json; then
            git add data.json health.json
            COMMIT_MSG="chore(data): ${{ steps.decide.outputs.run_label }}"
            git commit -m "$COMMIT_MSG"
            git pull --rebase
            git push
          else
            echo "No data changes to commit."
          fi

      - name: Post run summary
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let health = {};
            try { health = JSON.parse(fs.readFileSync('health.json', 'utf8')); } catch {}
            const ok = health.ok === true;
            const total = health.totalListings ?? 'n/a';
            const rate = health.successRatePct ?? 'n/a';
            const reason = health.reason ?? 'n/a';
            const artifacts = `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`;
            const mode = `${{ steps.decide.outputs.mode }}`;
            const body = [
              `Mode: ${mode}`,
              `Run: ${ok ? 'OK' : 'NOT OK'}`,
              `Listings: ${total}`,
              `Source success rate: ${rate}%`,
              `Reason: ${reason}`,
              `Artifacts: ${artifacts}`
            ].join('\n');
            core.summary
              .addHeading(`Vacancies Data Pipeline (${mode})`)
              .addRaw(body, true)
              .write();

      - name: Open issue if health not OK (scheduled only)
        if: github.event_name == 'schedule' && steps.health.outcome == 'success'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            let health = {};
            try { health = JSON.parse(fs.readFileSync('health.json', 'utf8')); } catch {}
            if (health.ok === true) { core.info('Health OK; skipping issue.'); return; }
            const artifacts = `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`;
            const mode = `${{ steps.decide.outputs.mode }}`;
            const title = `[Data Pipeline][${mode}] Health not OK - ${new Date().toISOString()}`;
            const body = [
              `Health status: ${JSON.stringify(health, null, 2)}`,
              '',
              `Artifacts (data.json, health.json): ${artifacts}`,
            ].join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['pipeline','investigate']
            });

  lighthouse:
    runs-on: ubuntu-latest
    needs: pipeline
    if: ${{ vars.PAGES_URL != '' }}
    steps:
      - name: Wait for Pages cache (retry)
        shell: bash
        run: |
          for i in {1..5}; do
            echo "Sleep $i..."
            sleep 10
          done
      - name: Audit homepage with Lighthouse CI
        uses: treosh/lighthouse-ci-action@v12
        with:
          urls: |
            ${{ vars.PAGES_URL }}
          budgetPath: ./.github/lighthouse/budget.json
          uploadArtifacts: true
      - name: Check health.json endpoint (retry)
        shell: bash
        run: |
          set -e
          base="${{ vars.PAGES_URL }}"; base="${base%/}"
          url="$base/health.json"
          for i in {1..5}; do
            code=$(curl -s -o /dev/null -w "%{http_code}" "$url")
            if [ "$code" = "200" ]; then
              echo "health.json OK (HTTP $code)"
              exit 0
            fi
            echo "health.json not ready (HTTP $code); retry $i/5"
            sleep 5
          done
          echo "health.json check failed after retries"
          exit 1
